{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimg_t = np.transpose(images[0], (1, 2, 0))\\n\\nplt.imshow(img_t)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.ops import roi_align\n",
    "from pytorchcv.models.common import SEBlock\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_per_process_memory_fraction(.9, device=device)\n",
    "else:\n",
    "    device = torch.device('cpu')         \n",
    "\n",
    "print(device)\n",
    "\n",
    "class CopySingleChannels(object):\n",
    "    ''' Does nothing to the image if it has three channels. Creates 3 copies if it has one channel. Returns assertion error if neither.'''\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        assert (image.shape[0] == 1 or image.shape[0] == 3)\n",
    "\n",
    "        if image.shape[0] == 1:       \n",
    "            return image.repeat(3, 1, 1)\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    torchvision.transforms.RandomAffine(degrees=15, translate= (.15, .15), scale = (0.85, 1.15)),\n",
    "    transforms.Resize((224,224)), #Resize data to be 224x224.\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(CopySingleChannels()) #transforms both RGB and grayscale to same tensor size by stacking copies of grayscales along first axis.\n",
    "    \n",
    "    ])\n",
    "\n",
    "\n",
    "# Get the data.\n",
    "full_dataset = torchvision.datasets.Caltech256(root='./data', download=True, transform=transform)\n",
    "# Get the indices of the data for splitting it later.\n",
    "full_indices = full_dataset.index\n",
    "\n",
    "\n",
    "\n",
    "# Generate breaks to generate a list of lists containing indices for each category.\n",
    "brks = [i for i in range(1,len(full_indices)) if full_indices[i] < full_indices[i-1]]\n",
    "split_indices = [full_indices[x:y] for x,y in zip([0]+brks,brks+[None])]\n",
    "\n",
    "# We do not want to include the 257th category, clutter, because they don't use it in the paper, so \n",
    "split_indices = split_indices[:256]\n",
    "\n",
    "# Get 60 random indices from each category.\n",
    "rand_indices = [sorted(random.sample(i, k = 60)) for i in split_indices]\n",
    "# Concatenate the list of lists to a list.\n",
    "train_indices = list(itertools.chain.from_iterable(rand_indices))\n",
    "\n",
    "assert len(train_indices) == 256*60\n",
    "\n",
    "# Get the indices not included in the train indices.\n",
    "get_test_indices = [list(set(split_indices[i]) - set(rand_indices[i])) for i in range(len(split_indices))]\n",
    "# Concatenate the list of lists to a list.\n",
    "test_indices = list(itertools.chain.from_iterable(get_test_indices))\n",
    "\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "\n",
    "test_dataset = torch.utils.data.Subset(full_dataset, test_indices)\n",
    "\n",
    "\n",
    "batch_sze = 8\n",
    "batched_train_data = torch.utils.data.DataLoader(train_dataset, batch_size = batch_sze, shuffle = True)\n",
    "\n",
    "\n",
    "#images, labels = next(iter(batched_train_data))\n",
    "\"\"\"\n",
    "img_t = np.transpose(images[0], (1, 2, 0))\n",
    "\n",
    "plt.imshow(img_t)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Intuitive Mechanism Behind SIFT Clustering.</h3>\n",
    "\n",
    "The code below shows how we came to arrive at the function \"get_SR_coords()\" later defined. Note that although this code shows the regions on the original image, the regions are actually applied to the output of the backbone CNN, in this case Resnet50, through RoI_Align()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_SRs_and_other_info(image_batch):\\n\\n    # Convert a single image to numpy array of expected shape for cv inputs\\n    im2 = image_batch[0].detach().numpy().transpose(1,2,0)\\n    # Rescale data for expected type \\n    im2 = (im2*255).astype(np.uint8)\\n\\n    \\n    sift = cv.SIFT_create()\\n    kp, desc = sift.detectAndCompute(im2,None)\\n\\n    # Sort keypoints by their response in descending order\\n    keypoints_lst = sorted(kp, key=lambda x: x.response, reverse=True)\\n\\n    # Apply threshold implicitly by resticting to top 50% keypoints.\\n    num_keypoints_to_keep = int(.5*len(keypoints_lst))\\n    keypoints = keypoints_lst[:num_keypoints_to_keep]\\n    print(num_keypoints_to_keep)\\n    \\n    # Option to include associated descriptors\\n    #filtered_descriptors = np.array([desc[i] for i in range(len(keypoints)) if i < num_keypoints_to_keep])\\n\\n    # Convert Keypoints to np Array\\n    pts = cv.KeyPoint_convert(keypoints)\\n\\n\\n    # Let kappa = 8 as the paper suggests.\\n    kappa = 8\\n\\n    # Gaussian Mixture Model\\n    gmm_pts = GMM(n_components= kappa, covariance_type= \\'full\\', init_params= \\'kmeans\\').fit(pts)\\n    gmm_labels = gmm_pts.predict(pts)\\n\\n\\n    # Sort pts into clusters\\n    clusters = defaultdict(list)\\n    for pt, label in zip(pts, gmm_labels):\\n        clusters[label].append(pt)\\n\\n    # Compute min and max values for each cluster to get the bounding boxes\\n    Prim_SRs = {}\\n    for label, points in clusters.items():\\n        xpts, ypts = zip(*points)\\n        Prim_SRs[label] = {\\n            \\'min_x\\': min(xpts),\\n            \\'max_x\\': max(xpts),\\n            \\'min_y\\': min(ypts),\\n            \\'max_y\\': max(ypts), \\n            \\'Primary_SR\\': image_batch[0,:,int(min(ypts)):int(max(ypts)),int(min(xpts)):int(max(xpts))]\\n    }\\n\\n\\n    # Note that we can get the primary SR\\'s by \"diagonal\" elements    \\n    Secon_SRs = {}\\n    for i, itemi in Prim_SRs.items():\\n        for j, itemj in Prim_SRs.items():\\n            if (i <= j):\\n                Secon_SRs[(i,j)] = {\\n                    \\'Secondary_SR\\': image_batch[0,:,int(min(itemi[\\'min_y\\'], itemj[\\'min_y\\'])):int(max(itemi[\\'max_y\\'], itemj[\\'max_y\\'])),int(min(itemi[\\'min_x\\'], itemj[\\'min_x\\'])):int(max(itemi[\\'max_x\\'], itemj[\\'max_x\\']))]\\n                }\\n            \\n\\n\\n    return pts, clusters, Secon_SRs\\n\\n\\n\\npts, clusters, SR_fam = get_SRs_and_other_info(images)\\n\\nplt.imshow(img_t)\\nplt.title(f\"{full_dataset.categories[labels[0]]}\")\\nxpts, ypts = zip(*pts)\\n#plt.scatter(xpts, ypts, c = gmm_labels)\\n\\n# Get points within first cluster\\n\\n\\nSR = SR_fam[(0,1)][\\'Secondary_SR\\']\\n\\n\\nclust1 = clusters[0]\\nx1pts, y1pts = zip(*clust1)\\nplt.scatter(x1pts, y1pts, c = \\'b\\')\\nclust2 = clusters[1]\\nx2pts, y2pts = zip(*clust2)\\nplt.scatter(x2pts, y2pts, c = \\'r\\')\\n\\n\\nplt.show()\\n\\ncrop_t = np.transpose(SR, (1, 2, 0))\\nplt.imshow(crop_t)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Note that you input the image batch output from Resnet in the actual implmentation of getting the SR's. This is just for conceptual purposes. \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def get_SRs_and_other_info(image_batch):\n",
    "\n",
    "    # Convert a single image to numpy array of expected shape for cv inputs\n",
    "    im2 = image_batch[0].detach().numpy().transpose(1,2,0)\n",
    "    # Rescale data for expected type \n",
    "    im2 = (im2*255).astype(np.uint8)\n",
    "\n",
    "    \n",
    "    sift = cv.SIFT_create()\n",
    "    kp, desc = sift.detectAndCompute(im2,None)\n",
    "\n",
    "    # Sort keypoints by their response in descending order\n",
    "    keypoints_lst = sorted(kp, key=lambda x: x.response, reverse=True)\n",
    "\n",
    "    # Apply threshold implicitly by resticting to top 50% keypoints.\n",
    "    num_keypoints_to_keep = int(.5*len(keypoints_lst))\n",
    "    keypoints = keypoints_lst[:num_keypoints_to_keep]\n",
    "    print(num_keypoints_to_keep)\n",
    "    \n",
    "    # Option to include associated descriptors\n",
    "    #filtered_descriptors = np.array([desc[i] for i in range(len(keypoints)) if i < num_keypoints_to_keep])\n",
    "\n",
    "    # Convert Keypoints to np Array\n",
    "    pts = cv.KeyPoint_convert(keypoints)\n",
    "\n",
    "\n",
    "    # Let kappa = 8 as the paper suggests.\n",
    "    kappa = 8\n",
    "\n",
    "    # Gaussian Mixture Model\n",
    "    gmm_pts = GMM(n_components= kappa, covariance_type= 'full', init_params= 'kmeans').fit(pts)\n",
    "    gmm_labels = gmm_pts.predict(pts)\n",
    "\n",
    "\n",
    "    # Sort pts into clusters\n",
    "    clusters = defaultdict(list)\n",
    "    for pt, label in zip(pts, gmm_labels):\n",
    "        clusters[label].append(pt)\n",
    "\n",
    "    # Compute min and max values for each cluster to get the bounding boxes\n",
    "    Prim_SRs = {}\n",
    "    for label, points in clusters.items():\n",
    "        xpts, ypts = zip(*points)\n",
    "        Prim_SRs[label] = {\n",
    "            'min_x': min(xpts),\n",
    "            'max_x': max(xpts),\n",
    "            'min_y': min(ypts),\n",
    "            'max_y': max(ypts), \n",
    "            'Primary_SR': image_batch[0,:,int(min(ypts)):int(max(ypts)),int(min(xpts)):int(max(xpts))]\n",
    "    }\n",
    "\n",
    "\n",
    "    # Note that we can get the primary SR's by \"diagonal\" elements    \n",
    "    Secon_SRs = {}\n",
    "    for i, itemi in Prim_SRs.items():\n",
    "        for j, itemj in Prim_SRs.items():\n",
    "            if (i <= j):\n",
    "                Secon_SRs[(i,j)] = {\n",
    "                    'Secondary_SR': image_batch[0,:,int(min(itemi['min_y'], itemj['min_y'])):int(max(itemi['max_y'], itemj['max_y'])),int(min(itemi['min_x'], itemj['min_x'])):int(max(itemi['max_x'], itemj['max_x']))]\n",
    "                }\n",
    "            \n",
    "\n",
    "\n",
    "    return pts, clusters, Secon_SRs\n",
    "\n",
    "\n",
    "\n",
    "pts, clusters, SR_fam = get_SRs_and_other_info(images)\n",
    "\n",
    "plt.imshow(img_t)\n",
    "plt.title(f\"{full_dataset.categories[labels[0]]}\")\n",
    "xpts, ypts = zip(*pts)\n",
    "#plt.scatter(xpts, ypts, c = gmm_labels)\n",
    "\n",
    "# Get points within first cluster\n",
    "\n",
    "\n",
    "SR = SR_fam[(0,1)]['Secondary_SR']\n",
    "\n",
    "\n",
    "clust1 = clusters[0]\n",
    "x1pts, y1pts = zip(*clust1)\n",
    "plt.scatter(x1pts, y1pts, c = 'b')\n",
    "clust2 = clusters[1]\n",
    "x2pts, y2pts = zip(*clust2)\n",
    "plt.scatter(x2pts, y2pts, c = 'r')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "crop_t = np.transpose(SR, (1, 2, 0))\n",
    "plt.imshow(crop_t)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Showing Some Backbone</h3>\n",
    "\n",
    "We get our CNN backbone, Resnet50, weights from PyTorch, and thanks to their improved training algorithm, these weights are improved over the original Resnet50 weights, later likely allowing our model to get 98.3% accuracy, higher than the 96% mean in the paper. Note that a lot of the commented out sections were used for debugging and/or to be sure that each module matches the number of paramters from the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nres_feature_map = CustomResNet50(device).to(device)\\n\\n\\n# Outputs \"torch.Size([Batch_sze, 2048, 7, 7])\", so C = 256\\nresnet_output = res_feature_map(images.type(torch.FloatTensor).to(device))\\nprint(resnet_output.size())\\n\\nmodel_parameters = filter(lambda p: p.requires_grad, res_feature_map.parameters())\\nres_params = sum([np.prod(p.size()) for p in model_parameters])\\nprint(f\"{res_params:,}\")\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "    PyTorch has the architecture for ResNet50 already available with the best pre-trained ImageNet weights. \n",
    "    We use Pyotrch's improved training weights instead of the original weights to better capture the features.\n",
    "\"\"\"\n",
    "class CustomResNet50(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(CustomResNet50, self).__init__()\n",
    "        resnet50 = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT).to(device)\n",
    "        \n",
    "        # Extract the first 5 blocks (Note that these are the conv1, conv2_x,..., conv_5x layers).\n",
    "        self.get_blocks = nn.Sequential(*list(resnet50.children())[:8])\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.get_blocks(x)\n",
    "\n",
    "#res_map = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT).to(device)\n",
    "#print(list(res_map.children())[7])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "res_feature_map = CustomResNet50(device).to(device)\n",
    "\n",
    "\n",
    "# Outputs \"torch.Size([Batch_sze, 2048, 7, 7])\", so C = 256\n",
    "resnet_output = res_feature_map(images.type(torch.FloatTensor).to(device))\n",
    "print(resnet_output.size())\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, res_feature_map.parameters())\n",
    "res_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"{res_params:,}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Getting the Semantic Region Coordinates</h3>\n",
    "\n",
    "This uses a very similar mechanism to group clusters as demonstrated in the commented our cell. One modification is that we want to get the lower-left and upper-right coords of the bounding boxes to feed into RoI_Align later. Note that RoI Align uses bilinear interpolation, and this is the bilinear pooling of the features mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We now get the coordinates of the SR's. This takes as input the image batch and outputs a list of tensors of different sizes.\n",
    "\"\"\"\n",
    "\n",
    "def get_SRs_coords(image_batch, loc_device, kappa=8):\n",
    "    batch_SRs = []\n",
    "    region_counts = []\n",
    "\n",
    "    B, C, H, W = image_batch.size()\n",
    "\n",
    "    for k in range(B):\n",
    "        # Convert a single image to numpy array of expected shape for cv inputs\n",
    "        copy_tens = image_batch[k].clone().cpu()\n",
    "        im2 = copy_tens.detach().numpy().transpose(1, 2, 0)\n",
    "        # Rescale data for expected type for CV \n",
    "        im2 = (im2 * 255).astype(np.uint8)\n",
    "\n",
    "        sift = cv.SIFT_create()\n",
    "        kp, desc = sift.detectAndCompute(im2, None)\n",
    "\n",
    "        # Sort keypoints by their response in descending order\n",
    "        keypoints_lst = sorted(set(kp), key=lambda x: x.response, reverse=True)\n",
    "\n",
    "        Secon_SRs = []\n",
    "        # Regardless of how many kp are detected, we want to include the whole image.\n",
    "        Secon_SRs.append(torch.tensor([0, H - 1, 0, W - 1]))\n",
    "\n",
    "        if len(keypoints_lst) > 1:\n",
    "            num_keypoints_to_keep = max(int(.5 * len(keypoints_lst)), 1)\n",
    "            if len(keypoints_lst) < 9:\n",
    "                num_keypoints_to_keep = len(keypoints_lst)\n",
    "                kappa = 2\n",
    "\n",
    "            keypoints = keypoints_lst[:num_keypoints_to_keep]\n",
    "            pts = cv.KeyPoint_convert(keypoints)\n",
    "\n",
    "            unique_pts = np.unique(pts, axis=0)\n",
    "            kappa_loc = min(kappa, len(unique_pts))\n",
    "\n",
    "            if kappa_loc > 1:  # Only fit GMM if there are at least 2 unique points\n",
    "                gmm_pts = GMM(n_components=kappa_loc, covariance_type='full', init_params='kmeans').fit(unique_pts)\n",
    "                gmm_labels = gmm_pts.predict(pts)\n",
    "\n",
    "                # Sort pts into clusters\n",
    "                clusters = defaultdict(list)\n",
    "                for pt, label in zip(pts, gmm_labels):\n",
    "                    clusters[label].append(pt)\n",
    "\n",
    "                # Compute min and max values for x and y coords of each cluster to get the bounding boxes\n",
    "                Prim_SRs = {}\n",
    "                for label, points in clusters.items():\n",
    "                    xpts, ypts = zip(*points)\n",
    "                    Prim_SRs[label] = {\n",
    "                        'min_x': min(xpts),\n",
    "                        'max_x': max(xpts),\n",
    "                        'min_y': min(ypts),\n",
    "                        'max_y': max(ypts),\n",
    "                    }\n",
    "\n",
    "                # Note that we can get the primary SR's via \"diagonal\" elements (when i = j )   \n",
    "                for i, itemi in Prim_SRs.items():\n",
    "                    for j, itemj in Prim_SRs.items():\n",
    "                        if i <= j:\n",
    "                            x1, x2 = min(itemi['min_y'], itemj['min_y']), max(itemi['max_y'], itemj['max_y'])\n",
    "                            y1, y2 = min(itemi['min_x'], itemj['min_x']), max(itemi['max_x'], itemj['max_x'])\n",
    "\n",
    "                            Secon_SRs.append(torch.tensor([x1, y1, x2, y2]))\n",
    "\n",
    "        tens = torch.stack(Secon_SRs).to(loc_device)\n",
    "        batch_SRs.append(tens)\n",
    "        region_counts.append(tens.size(0))\n",
    "\n",
    "    return batch_SRs, region_counts\n",
    "\n",
    "# List of size B X different R+1 X 4\n",
    "#SR_list, R_counts = get_SRs_coords(images, device, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating the Necessary Modules </h3>\n",
    "\n",
    "We implement AG-Net by building it piece-by-piece then assembling into one Torch module later for training. Note that if you use a different backbone CNN, you will possibly need to adjust the scale of the RoI and the number of input channels. As before, we leave some of the commented-out code to \"show our work\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Given the feature maps, we now want to begin the intra-attention mechanism. \"\"\"\n",
    "\n",
    "class IntraSelfAttn(nn.Module):\n",
    "    \"\"\" \n",
    "        Self Attention Layer adapted from SAGAN Implementation available at https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels = 2048, B = 8):\n",
    "        super(IntraSelfAttn,self).__init__()\n",
    "        self.chanel_in = in_channels\n",
    "        \n",
    "        self.query_conv = nn.Conv2d(in_channels = in_channels , out_channels = in_channels//8 , kernel_size= 1) # f(x)\n",
    "        self.key_conv = nn.Conv2d(in_channels = in_channels , out_channels = in_channels//8 , kernel_size= 1) # g(x)\n",
    "        self.value_conv = nn.Conv2d(in_channels = in_channels , out_channels = in_channels , kernel_size= 1) # h(x)\n",
    "        self.delta = nn.Parameter(torch.zeros(1)) # Delta initialized to be 0\n",
    "\n",
    "        self.softmax  = nn.Softmax(dim=-1) \n",
    "     def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps (B x C x W x H)\n",
    "            returns :\n",
    "                out : self attention value + input feature (same size)\n",
    "        \"\"\"\n",
    "        B,C,width ,height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(B,-1,width*height).permute(0,2,1) # B x C x W*H\n",
    "        proj_key =  self.key_conv(x).view(B,-1,width*height) # B x C x W*H\n",
    "        energy =  torch.bmm(proj_query,proj_key) \n",
    "        attention = self.softmax(energy) # B x W*H x W*H \n",
    "        proj_value = self.value_conv(x).view(B,-1,width*height) # B x C x W*H\n",
    "\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) ).view(B,C,width,height)\n",
    "        \n",
    "        out = self.delta*out + x\n",
    "        return out\n",
    "\n",
    "#Intra = IntraSelfAttn().to(device)\n",
    "#Intra_out = Intra(resnet_output)\n",
    "#model_parameters = filter(lambda p: p.requires_grad, Intra.parameters())\n",
    "#intra_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "#print(f\"Intra params: {intra_params:,}\")\n",
    "#print(Intra_out.size())\n",
    "\n",
    "\"\"\"  \n",
    "roi_align will take as input the Intra-self attention features and \n",
    "an input parameter boxes which takes in a list of box tensors [batch_idx, x1, y1, x2, y2]. \n",
    "obtained with get_SRs(). \n",
    "If a batch is passed, we must have that the first column of boxes contains the \n",
    "index of the corresponding element in the batch.\n",
    "\n",
    "\n",
    "Returns a tensor of size sum(R_i + 1) x output_size x output_size, where R_i is the number of SR's detected in image i from a batch.\n",
    "\"\"\"\n",
    "\n",
    "# SR_list gets coordinates from 224x224 images, but we want to assign them to 56 x 56 since these are the Resnet50 output dimensions.\n",
    "#scale = 7/224\n",
    "\n",
    "#roi = roi_align(Intra_out, SR_list, output_size= 7, spatial_scale= scale).to(device)\n",
    "\n",
    "#print(roi.size())\n",
    "\n",
    "\n",
    "\n",
    "class SE_Residual(nn.Module):\n",
    "\n",
    "    \"\"\" Takes in batch of bilinearly pooled features. Outputs tensor of same size. Rp1 = R + 1. \"\"\"\n",
    "    def __init__(self, channels = 2048, Rp1 = 37):\n",
    "        super(SE_Residual, self).__init__()\n",
    "        self.SE = nn.ModuleList([SEBlock(channels=channels, reduction=16) for i in range(Rp1)])\n",
    "\n",
    "    def forward(self, feature_tens, region_counts):\n",
    "        \n",
    "        B = len(region_counts)\n",
    "        \n",
    "        \n",
    "        feature_list = torch.split(feature_tens, region_counts, dim = 0)\n",
    "        outputs = []\n",
    "        for i in range(B):\n",
    "            for j in range(region_counts[i]):\n",
    "                x = self.SE[j](feature_list[i][j])\n",
    "                resid = x + feature_list[i][j].unsqueeze(0)\n",
    "                outputs.append(resid)\n",
    "                \n",
    "\n",
    "        return torch.concat(outputs, dim=0)\n",
    "\n",
    "#channel_num = roi.size()[1]\n",
    "#SE_res = SE_Residual().to(device)\n",
    "#model_parameters = filter(lambda p: p.requires_grad, SE_res.parameters())\n",
    "#se_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "#print(f\"SE res params: {se_params:,}\")\n",
    "#SE_out = SE_res(roi, R_counts)\n",
    "\n",
    "\n",
    "class InterSelfAttn(nn.Module):\n",
    "    def __init__(self, in_channels = 2048, B = 8):\n",
    "        super(InterSelfAttn, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # Shared 1x1 convolutions for parameter efficiency\n",
    "        self.W_u = torch.nn.ModuleList([nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)  for i in range(B)])\n",
    "        self.W_u_prime = torch.nn.ModuleList([nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)  for i in range(B)])\n",
    "        self.W_m = torch.nn.ModuleList([nn.Conv2d(in_channels // 8, 1, kernel_size=1)  for i in range(B)])\n",
    "        self.W_alpha = torch.nn.ModuleList([nn.Conv2d(in_channels, 1, kernel_size=1)  for i in range(B)])\n",
    "        \n",
    "    def forward(self, feature_tens, region_counts):\n",
    "        B = len(region_counts)\n",
    "        r = feature_tens.size(-1)\n",
    "\n",
    "        feature_list = torch.split(feature_tens, region_counts)\n",
    "        \n",
    "        \n",
    "        outputs = []\n",
    "        for i in range(B):\n",
    "            R = region_counts[i]\n",
    "            regions = feature_list[i]  # R x C x H x W\n",
    "            \n",
    "            \n",
    "            # Compute u_{r,r'}\n",
    "            u_r = self.W_u[i](regions).unsqueeze(0)  # 1 x R x C/8 x H x W\n",
    "            u_r_prime = self.W_u_prime[i](regions).unsqueeze(1)  # R x 1 x C/8 x H x W\n",
    "            u_r_r_prime = torch.tanh(u_r + u_r_prime)  # R x R x C/8 x H x W\n",
    "            \n",
    "            # Compute m_{r,r'}\n",
    "            m_r_r_prime = self.W_m[i](u_r_r_prime.view(R * R, -1, r, r)).view(R, R, 1, r, r)  # R x R x 1 x H x W \n",
    "            m_r_r_prime = torch.sigmoid(m_r_r_prime)\n",
    "            \n",
    "            # Aggregate attentional features\n",
    "            alpha_r = torch.sum(m_r_r_prime * regions.unsqueeze(1), dim=0)  # R x C x H x W\n",
    "            \n",
    "            # Compute weights w_r\n",
    "            w_r = self.W_alpha[i](alpha_r).view(R, -1)  \n",
    "            w_r = F.softmax(w_r, dim=0).view(R, 1, r, r)  # R x 1 x H x W\n",
    "            \n",
    "            # Combine regional features\n",
    "            f_hat = torch.sum(alpha_r * w_r, dim=0)  # C x H x W\n",
    "            outputs.append(f_hat)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=0)  # B x C x H x W\n",
    "        \n",
    "        return outputs\n",
    "#k = 5\n",
    "#kappa_max = k*(k+1)//2 + 1\n",
    "\n",
    "#print(kappa_max)\n",
    "#inter = InterSelfAttn().to(device)\n",
    "\n",
    "\n",
    "#model_parameters = filter(lambda p: p.requires_grad, inter.parameters())\n",
    "#inter_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "#print(f\"Inter params: {inter_params:,}\")\n",
    "#inter_out = inter(SE_out, R_counts)\n",
    "#print(inter_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\nc = Classify().to(device)\\nc_out = c(inter_out)\\n#print(c_out.size())\\nmodel_parameters = filter(lambda p: p.requires_grad, c.parameters())\\nclassify_params = sum([np.prod(p.size()) for p in model_parameters])\\nprint(f\"Classify params: {classify_params:,}\")\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classify(nn.Module):\n",
    "    def __init__(self, C = 2048, num_classes = 256):\n",
    "        super(Classify, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        # Layers for classification\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.gmp = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        \n",
    "        self.Womega = nn.Linear(C, 1)\n",
    "        self.bomega = nn.Parameter(torch.zeros(1))\n",
    "        self.fc = nn.Linear(C, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            inputs:\n",
    "                x: input batch of features (B x C x 7 x 7)\n",
    "            returns:\n",
    "                out: class probabilities (B x num_classes)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        # Apply GAP and GMP\n",
    "        f_gap = self.gap(x).view(B, C)\n",
    "        f_gmp = self.gmp(x).view(B, C)\n",
    "\n",
    "        # Calculate omega and combine features\n",
    "        omega = self.softmax(self.Womega(f_gap) + self.bomega)\n",
    "        F = omega * f_gmp + (1 - omega) * f_gap\n",
    "        \n",
    "        # Classification\n",
    "        class_probs = self.fc(F)\n",
    "\n",
    "        return class_probs\n",
    "        \n",
    "\"\"\"    \n",
    "c = Classify().to(device)\n",
    "c_out = c(inter_out)\n",
    "#print(c_out.size())\n",
    "model_parameters = filter(lambda p: p.requires_grad, c.parameters())\n",
    "classify_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"Classify params: {classify_params:,}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"Resnet params: {res_params:,}\")\\n\\ntotal_params_baseline = classify_params + res_params \\n# Should be around 23.62M, but our count is 24.03M \\nprint(f\"Baseline params: {total_params_baseline:,}\")\\n\\n# Should be around 43.02M\\ntotal_params_no_attn = total_params_baseline + se_params  \\nprint(f\"-Attn params: {total_params_no_attn:,}\")\\n\\n# Should be around 54.79M (not including last layers)\\ntotal_params = total_params_no_attn + inter_params+ intra_params\\nprint(f\"+Attn params: {total_params:,}\")\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(f\"Resnet params: {res_params:,}\")\n",
    "\n",
    "total_params_baseline = classify_params + res_params \n",
    "# Should be around 23.62M, but our count is 24.03M \n",
    "print(f\"Baseline params: {total_params_baseline:,}\")\n",
    "\n",
    "# Should be around 43.02M\n",
    "total_params_no_attn = total_params_baseline + se_params  \n",
    "print(f\"-Attn params: {total_params_no_attn:,}\")\n",
    "\n",
    "# Should be around 54.79M\n",
    "total_params = total_params_no_attn + inter_params+ intra_params\n",
    "print(f\"+Attn params: {total_params:,}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Putting Everything Together </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AG_Net(nn.Module):\n",
    "    def __init__(self, kappa, batch_size_loc, device_loc):\n",
    "        super(AG_Net, self).__init__()\n",
    "        self.dev = device_loc\n",
    "        self.batch = batch_size_loc\n",
    "        self.kap = kappa\n",
    "        self.kappa_max = 1+kappa*(kappa+1)//2\n",
    "        self.c_res = CustomResNet50(device).to(device)\n",
    "        self.intra = IntraSelfAttn().to(device_loc)\n",
    "        self.se_res = SE_Residual(channels= 2048, Rp1= self.kappa_max).to(device_loc)\n",
    "        self.inter = InterSelfAttn(B= batch_size_loc).to(device_loc)\n",
    "        self.classify = Classify().to(device_loc)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res_feats = self.c_res(x)\n",
    "        SR_list, region_counts = get_SRs_coords(x, self.dev, self.kap)\n",
    "        x = self.intra(res_feats)\n",
    "        x = roi_align(x, SR_list, output_size= 7, spatial_scale= 7/224).to(device)\n",
    "        x = self.se_res(x, region_counts)\n",
    "        x = self.inter(x, region_counts)\n",
    "        x = self.classify(x)        \n",
    "        return x\n",
    "\n",
    "kappa_global = 8\n",
    "net = AG_Net(kappa = kappa_global, batch_size_loc=batch_sze, device_loc= device).to(device)\n",
    "#net_out = net(SR_tens)\n",
    "\n",
    "\n",
    "#model_parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
    "#params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "#print(f\"{params:,}\")\n",
    "\n",
    "#Should be about 56.79M\n",
    "#print(f\"{params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training the Model (Possibly From Checkpoints)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> no checkpoint found at 'AG_net_weights2.pth'\n",
      "[1,  1920] loss: 0.496\n",
      "[2,  1920] loss: 0.029\n",
      "[3,  1920] loss: 0.017\n",
      "[4,  1920] loss: 0.009\n",
      "[5,  1920] loss: 0.009\n",
      "[6,  1920] loss: 0.007\n",
      "[7,  1920] loss: 0.003\n",
      "[8,  1920] loss: 0.007\n",
      "[9,  1920] loss: 0.003\n",
      "[10,  1920] loss: 0.003\n",
      "[11,  1920] loss: 0.003\n",
      "[12,  1920] loss: 0.004\n",
      "[13,  1920] loss: 0.002\n",
      "[14,  1920] loss: 0.002\n",
      "[15,  1920] loss: 0.002\n",
      "[16,  1920] loss: 0.002\n",
      "[17,  1920] loss: 0.002\n",
      "[18,  1920] loss: 0.001\n",
      "[19,  1920] loss: 0.001\n",
      "[20,  1920] loss: 0.004\n",
      "[21,  1920] loss: 0.001\n",
      "[22,  1920] loss: 0.001\n",
      "[23,  1920] loss: 0.002\n",
      "[24,  1920] loss: 0.001\n",
      "[25,  1920] loss: 0.001\n",
      "[26,  1920] loss: 0.001\n",
      "[27,  1920] loss: 0.000\n",
      "[28,  1920] loss: 0.001\n",
      "[29,  1920] loss: 0.001\n",
      "[30,  1920] loss: 0.001\n",
      "[31,  1920] loss: 0.001\n",
      "[32,  1920] loss: 0.001\n",
      "[33,  1920] loss: 0.002\n",
      "[34,  1920] loss: 0.000\n",
      "[35,  1920] loss: 0.001\n",
      "[36,  1920] loss: 0.001\n",
      "[37,  1920] loss: 0.001\n",
      "[38,  1920] loss: 0.001\n",
      "[39,  1920] loss: 0.001\n",
      "[40,  1920] loss: 0.001\n",
      "[41,  1920] loss: 0.000\n",
      "[42,  1920] loss: 0.001\n",
      "[43,  1920] loss: 0.001\n",
      "[44,  1920] loss: 0.000\n",
      "[45,  1920] loss: 0.001\n",
      "[46,  1920] loss: 0.002\n",
      "[47,  1920] loss: 0.001\n",
      "[48,  1920] loss: 0.001\n",
      "[49,  1920] loss: 0.001\n",
      "[50,  1920] loss: 0.001\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-5, momentum=0.99)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, train_indices, test_indices, device, filename='AG_net_weights2.pth'):\n",
    "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
    "    start_epoch = 0\n",
    "    loaded_flag = False\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        train_indices = checkpoint['train_indices']\n",
    "        test_indices = checkpoint['test_indices']\n",
    "        loaded_flag = True\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer, start_epoch, train_indices, test_indices, loaded_flag\n",
    "\n",
    "# Loading the saved model\n",
    " \n",
    "net, optimizer, start_epoch, train_indices, test_indices, loaded_flag = load_checkpoint(net, optimizer, train_indices, test_indices, device)\n",
    "net = net.to(device)\n",
    "\n",
    "if loaded_flag:\n",
    "    # now individually transfer the optimizer parts...\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(full_dataset, test_indices)\n",
    "    batched_train_data = torch.utils.data.DataLoader(train_dataset, batch_size = batch_sze, shuffle = True)\n",
    "\n",
    "net.train()\n",
    "end_epoch = 50\n",
    "\n",
    "for epoch in range(start_epoch, end_epoch):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    if (epoch > 23):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=1e-6, momentum=0.99)\n",
    "    for i, batch in enumerate(batched_train_data):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.type(torch.LongTensor).to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs).type(torch.FloatTensor).to(device)\n",
    "        #print(outputs, labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #loss.requires_grad = True\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        #\"\"\"\n",
    "        running_loss += loss.item()\n",
    "        print_num = 1920\n",
    "        if i % print_num == (print_num - 1):    # print every print_num batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_num:.3f}')\n",
    "            running_loss = 0.0\n",
    "        #\"\"\"\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Saving Training Progress </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Define the filename for saving the model\n",
    "filename = 'AG_net_weights2.pth'\n",
    "\n",
    "# Construct the full path to save the model\n",
    "save_path = os.path.join(notebook_dir, filename)\n",
    "\n",
    "state = {'epoch': epoch + 1, 'state_dict': net.state_dict(),\n",
    "             'optimizer': optimizer.state_dict(), 'train_indices': train_indices, 'test_indices': test_indices}\n",
    "torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Testing the Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset: 98.34%\n"
     ]
    }
   ],
   "source": [
    "# Define a DataLoader for the test dataset\n",
    "batch_size_test = 8\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation during evaluation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        blah, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        \n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the test dataset: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
